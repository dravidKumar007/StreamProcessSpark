{
  "metadata": {
    "name": "Spark kafka \u0026 Cassandra",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\n\nval spark \u003d SparkSession.builder()\n  .appName(\"Spark Cassandra\")\n  .config(\"spark.cassandra.connection.host\", \"0.0.0.0\")\n  .config(\"spark.cassandra.connection.port\", \"9042\")\n  .getOrCreate()\n\nval df1 \u003d spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n  .option(\"subscribe\", \"userVisited\")\n  .load()\n\nval kafkaData \u003d df1\n  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n  .withColumn(\"value\", regexp_replace(col(\"value\"), \"\\\"\", \"\"))\n  .withColumn(\"value\", split(col(\"value\"), \",\"))\n  .select(\n    col(\"value\").getItem(0).as(\"customer_email\"),\n    col(\"value\").getItem(1).as(\"product_id\")\n  )\n  .withColumn(\"product_id\", col(\"product_id\").cast(\"int\"))\n  .withColumn(\"discount\", lit(0))\n  .withColumn(\"visit_count\", lit(1))\n\nval df \u003d spark.read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map(\"table\" -\u003e \"customer_visits\", \"keyspace\" -\u003e \"customervists\"))\n  .load()\n\nval dfWithLongId \u003d df.withColumn(\"id\", col(\"id\").cast(\"long\"))\n\nval maxIdRow \u003d dfWithLongId.agg(max(\"id\").as(\"max_id\")).collect()(0)\nval maxId \u003d if (maxIdRow.isNullAt(0)) 0L else maxIdRow.getAs[Long](\"max_id\")\n\nval newDf \u003d kafkaData\n  .withColumn(\"id\", expr(s\"$maxId + monotonically_increasing_id() + 1\"))\n\nnewDf.createOrReplaceTempView(\"new_data\")\ndfWithLongId.createOrReplaceTempView(\"existing_data\")\n\nval updatedData \u003d spark.sql(\"\"\"\n  SELECT\n    COALESCE(e.id, n.id) AS id,\n    COALESCE(e.customer_email, n.customer_email) AS customer_email,\n    COALESCE(n.product_id, e.product_id) AS product_id,\n    COALESCE(e.visit_count, 0) + COALESCE(n.visit_count, 1) AS visit_count,\n    COALESCE(e.discount, 0) AS discount\n  FROM\n    new_data n\n  FULL OUTER JOIN\n    existing_data e\n  ON\n    e.customer_email \u003d n.customer_email\n\"\"\")\n\nval updatedWithDiscount \u003d updatedData.withColumn(\"discount\",\n  when(col(\"visit_count\") \u003e 46, 60)\n    .otherwise(\n      (5 + (floor((col(\"visit_count\") - 1) / 4) * 5)).cast(\"int\")\n    )\n)\n\nupdatedWithDiscount.write\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map(\"table\" -\u003e \"customer_visits\", \"keyspace\" -\u003e \"customervists\"))\n  .mode(\"append\")\n  .save()\n\nval query \u003d kafkaData.writeStream\n  .outputMode(\"append\")\n  .format(\"console\")\n  .start()\n\nquery.awaitTermination()\n"
    }
  ]
}